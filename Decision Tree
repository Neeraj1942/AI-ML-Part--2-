When we split the entire data based on the features then that is a desicion tree.(split on height, split on performance, split on class)
Both continuos variables ( height, weight .. like numberical data) and discrete/categorical data ( colour gender... more of words) can be split by using decision trees.
Objective of decision tree - > to have pure nodes ( only one set of data with 100% accuracy)
A pure node has 100% of only one class type(variable)
Trminologies ->
1. Root node         -> (entire set of data) (everything)
2.Splitting          -> Dividing the initial root node into sub nodes
3.Decision Node      -> the first split nodes from the root node are called decision node
4.Leaf/Terminal Node -> Nodes which cannot be split further
5.Branch/Sub Tree    -> a subsection of a entire decision tree
6.Parent/child node  -> the node which splits to sub nodes is called child node , the initial node which got split up is called the parent node
7. Dept of a tree    -> the longest path from the (Root -> Leaf )node.

Selecting the best split ->

1.Gini impurity = 1-gini

Properties ->
Lower the gini impurity higher the homogenous nodes(high success rate)
Works only on categorical data (not on continuous data)
Performs only binary splits

How to apply -> 
1. calculate the probability of a.success b.failure
2. calculate the gini = (success)^2 + (failure)^2
3. gini impurity = 1- gini = 1- (success)^2 + (failure)^2
4. weighted gini impurity = (node1 items)/(total items) * gini impurity(node1) (node2 items)/(total items) * gini impurity(node2)

Example -> total students =20 -> split into 2 nodes based on above average->
Split on Performance in Class (20 students total)
Above Average (14 students)
	•	Play = 8, Not play = 6
	•	Prob(Play) = 0.57, Prob(Not) = 0.43
	•	Gini impurity = 1 − (0.57² + 0.43²) = 0.49

Below Average (6 students)
	•	Play = 2, Not play = 4
	•	Prob(Play) = 0.33, Prob(Not) = 0.67
	•	Gini impurity = 1 − (0.33² + 0.67²) = 0.44

Weighted Gini (Performance in Class)
(14/20)\cdot 0.49 + (6/20)\cdot 0.44 = 0.476
Result: Parent Gini = 0.50 → After split = 0.476 (slight improvement)

Finally if there is no impurity the chi square value is 1.
Lower Gini impurity = higher homogeneity -> best split
as gini decreaes - > the split is more different from the parent node

2. Chi Square ->
Higher Chi-square = higher homogeneity -> best split
as chi square increases - > the split is more different from the parent node

Properties ->
works on categorical data same like gini impurity (does not work on continuous split)
higher the chi-square value higher the homogenuity of the nodes

How to apply ->
1. check how to parent node(root node) is split (ex -50% ,50% or so on) 
2. the root node split is the expected split in the sub nodes
3. calculate the actual - expected
4. formula is root[(actual - expected)^2/expected]
5. now add all the values of both the nodes , and the success and not success for both.

Example ->
Parent Node
	•	Total Students = 20
	•	Play Cricket = 10, Not Play = 10
	•	Percentage Play = 50%

1) Child Node — Above Average (14 students)
	•	Expected Play = 14 \times 0.5 = 7
	•	Expected Not Play = 7
	•	Actual Play = 8, Actual Not Play = 6
$$
\chi^2_{\text{Play}} = \frac{(8-7)^2}{7} = \frac{1}{7} \approx 0.143
\chi^2_{\text{Not Play}} = \frac{(6-7)^2}{7} = \frac{1}{7} \approx 0.143
$$
Total (Above) = 0.143 + 0.143 = 0.286

2) Child Node — Below Average (6 students)
	•	Expected Play = 6 \times 0.5 = 3
	•	Expected Not Play = 3
	•	Actual Play = 2, Actual Not Play = 4

\chi^2_{\text{Play}} = \frac{(2-3)^2}{3} = \frac{1}{3} \approx 0.333
\chi^2_{\text{Not Play}} = \frac{(4-3)^2}{3} = \frac{1}{3} \approx 0.333
Total (Below) = 0.333 + 0.333 = 0.667

3) Chi-Square for Split (Performance in Class)

\chi^2 = 0.286 + 0.667 = 0.953 ( sum of all the cases)

✅ Result: Chi-Square value for splitting on Performance in Class = 0.953

3. Information Gain->
Again for categorical data
uses entropy as the major factor.

formula = 1- entropy(weighted entropy)

entropy = -(p)*log2(p), -> -(p1)*log2(p1) -(p2)*log2(p2) -(p3)*log2(p3)    .... -(pn)*log2(pn)
weighted entropy = p1/(p total) * -(p1)*log2(p1) + p2/(p of total) * -(p2)*log2(p2) ..... pn/(p of total) * -(pn)*log2(pn)
then we do 1-the value to get the Information gain.

Higher the information gain -> higher the homogenity 
incase the information gain of the parent node > child node ( child node is more homogenuos)



For contiuous target variables we use->
4. Reduction in variance :
This method invloves entropy
1. We calculate the entropy for the parent node and child node respectively
2. Then we can calculate the weighted entropies
3. Then we compare and select the one which has less entropy 

less entropy = more homogenuous

Reduction in variance = summation[X - Mean]^2/n

example ->
a. split in class ->
we have to change that to numerical , that is continuous data -> plays cricket = 1, no play cricket = 0 
Above Average (14 students)
	•	Play = 8, Not play = 6
	•	Prob(Play) = 0.57, Prob(Not) = 0.43
	•	Mean = (8*1+6*0)/14 = 0.57
	    Variance = [8*(1-0.57)^2 + 6*(0-0.57)^2]/14 = 0.245
		

Below Average (6 students)
	•	Play = 2, Not play = 4
	•	Prob(Play) = 0.33, Prob(Not) = 0.67
	•	Mean = (2*1+4*0)/6 = 0.33
	    Variance = [2*(1-0.33)^2 + 4*(0-0.33)^2]/6 = 0.222

		Weighted Variance (for both the split- classes) = 14/20 * 0.245 + 6/20 * 0.222 = 0.238

Class IX (10 students)
	•	Play = 8, Not play = 2
	•	Prob(Play) = 0.8, Prob(Not) = 0.2
	•	Mean = (8*1+2*0)/10 = 0.8
	    Variance = [8*(1-0.8)^2 + 2*(0-0.8)^2]/10 = 0.16
		

Class X (10 students)
	•	Play = 2, Not play = 8
	•	Prob(Play) = 0.2, Prob(Not) = 0.8
	•	Mean = (2*1+8*0)/10 = 0.2
	    Variance = [2*(1-0.2)^2 + 8*(0-0.2)^2]/10 = 0.16

		Weighted Variance (for both the split- classes) = 10/20 * 0.16 + 10/20 * 0.16 = 0.16
Now, comparing both the variance of Class Average, Class (IX,X)   -> 0.238 , 0.16 respectively so we select 0.16(Class IX,X) which is the less variance so we select this one.
